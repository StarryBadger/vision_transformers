{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptFkb1_Uvdoz",
        "outputId": "154ded6b-0f35-4c26-f08b-c36d038abca9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-r3ffzxhz\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-r3ffzxhz\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=949ca4f2284eee5232def2699e26dbd57c7e282054ea650087d97c642b77eaf9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wr04i53q/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/openai/CLIP.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEGjlg4AxbdL",
        "outputId": "52b93a2d-23b5-4b59-de30-497f5d49d00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Archive:  UTF-8sample_images.zip\n",
            "   creating: sample_images/\n",
            "  inflating: sample_images/bee_eater.JPEG  \n",
            "  inflating: sample_images/black_swan.JPEG  \n",
            "  inflating: sample_images/hen.JPEG  \n",
            "  inflating: sample_images/kite.JPEG  \n",
            "  inflating: sample_images/tiger_shark.JPEG  \n"
          ]
        }
      ],
      "source": [
        "# !unzip UTF-8sample_images.zip\n",
        "!unzip counteranimal.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fs1DeQfJvJhb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import clip\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import ImageNet\n",
        "import requests\n",
        "import torchvision\n",
        "from io import BytesIO\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx_gO4z3vJhd",
        "outputId": "698a207d-36cf-4033-8a79-98690d3000b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model comparison:\n",
            "ImageNet ResNet-50 architecture: <class 'torchvision.models.resnet.ResNet'>\n",
            "CLIP visual encoder architecture: <class 'clip.model.ModifiedResNet'>\n",
            "\n",
            "Architecture Comparison:\n",
            "ImageNet ResNet-50 layers:\n",
            "  conv1: <class 'torch.nn.modules.conv.Conv2d'>\n",
            "  bn1: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
            "  relu: <class 'torch.nn.modules.activation.ReLU'>\n",
            "  maxpool: <class 'torch.nn.modules.pooling.MaxPool2d'>\n",
            "  layer1: <class 'torch.nn.modules.container.Sequential'>\n",
            "  layer2: <class 'torch.nn.modules.container.Sequential'>\n",
            "  layer3: <class 'torch.nn.modules.container.Sequential'>\n",
            "  layer4: <class 'torch.nn.modules.container.Sequential'>\n",
            "  avgpool: <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>\n",
            "  fc: <class 'torch.nn.modules.linear.Linear'>\n",
            "\n",
            "CLIP visual encoder layers:\n",
            "  conv1: <class 'torch.nn.modules.conv.Conv2d'>\n",
            "  bn1: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
            "  relu1: <class 'torch.nn.modules.activation.ReLU'>\n",
            "  conv2: <class 'torch.nn.modules.conv.Conv2d'>\n",
            "  bn2: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
            "  relu2: <class 'torch.nn.modules.activation.ReLU'>\n",
            "  conv3: <class 'torch.nn.modules.conv.Conv2d'>\n",
            "  bn3: <class 'torch.nn.modules.batchnorm.BatchNorm2d'>\n",
            "  relu3: <class 'torch.nn.modules.activation.ReLU'>\n",
            "  avgpool: <class 'torch.nn.modules.pooling.AvgPool2d'>\n",
            "  layer1: <class 'torch.nn.modules.container.Sequential'>\n",
            "  layer2: <class 'torch.nn.modules.container.Sequential'>\n",
            "  layer3: <class 'torch.nn.modules.container.Sequential'>\n",
            "  layer4: <class 'torch.nn.modules.container.Sequential'>\n",
            "  attnpool: <class 'clip.model.AttentionPool2d'>\n"
          ]
        }
      ],
      "source": [
        "#############################################################\n",
        "# Task 1: Inference using CLIP and ImageNet pretrained ResNet-50\n",
        "#############################################################\n",
        "\n",
        "# Load ImageNet pretrained ResNet-50\n",
        "imagenet_model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
        "imagenet_model = imagenet_model.to(device)\n",
        "imagenet_model.eval()\n",
        "\n",
        "# Load CLIP model\n",
        "clip_model, clip_preprocess = clip.load(\"RN50\", device=device)\n",
        "clip_model.eval()\n",
        "\n",
        "# Extract CLIP's visual encoder\n",
        "clip_visual_encoder = clip_model.visual\n",
        "\n",
        "print(\"Model comparison:\")\n",
        "print(f\"ImageNet ResNet-50 architecture: {type(imagenet_model)}\")\n",
        "print(f\"CLIP visual encoder architecture: {type(clip_visual_encoder)}\")\n",
        "\n",
        "# Compare architectures\n",
        "def compare_architectures():\n",
        "    print(\"\\nArchitecture Comparison:\")\n",
        "    print(\"ImageNet ResNet-50 layers:\")\n",
        "    for name, module in imagenet_model.named_children():\n",
        "        print(f\"  {name}: {type(module)}\")\n",
        "\n",
        "    print(\"\\nCLIP visual encoder layers:\")\n",
        "    for name, module in clip_visual_encoder.named_children():\n",
        "        print(f\"  {name}: {type(module)}\")\n",
        "\n",
        "compare_architectures()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "h3RmTo7IvJhf"
      },
      "outputs": [],
      "source": [
        "#############################################################\n",
        "# Task 2: Setup data and understand ImageNet dataset\n",
        "#############################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCaflXcvvJhg",
        "outputId": "ae2f2fb6-0f95-4c8d-fe53-270cd0ec1974"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Testing models on sample image:\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'sample_images'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     53\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [(imagenet_classes[idx], values[i].item()) \u001b[38;5;28;01mfor\u001b[39;00m i, idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(indices)]\n\u001b[32m     57\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mTesting models on sample image:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msample_images\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     61\u001b[39m     img_path = os.path.join(\u001b[33m'\u001b[39m\u001b[33msample_images\u001b[39m\u001b[33m'\u001b[39m, path)\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mImage: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'sample_images'"
          ]
        }
      ],
      "source": [
        "#############################################################\n",
        "# Task 3: Setup zero-shot CLIP\n",
        "#############################################################\n",
        "\n",
        "\n",
        "imagenet_classes = torchvision.models.ResNet50_Weights.IMAGENET1K_V1.meta[\"categories\"]\n",
        "text_inputs = torch.cat([clip.tokenize(f\"a photo of a {c}\") for c in imagenet_classes]).to(device)\n",
        "\n",
        "def predict_with_clip(image_path):\n",
        "    # Load and preprocess image\n",
        "    image = clip_preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Get image features\n",
        "        image_features = clip_model.encode_image(image)\n",
        "\n",
        "        # Get text features for all ImageNet classes\n",
        "        text_features = clip_model.encode_text(text_inputs)\n",
        "\n",
        "        # Normalize features\n",
        "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Calculate similarity (cosine similarity as logits)\n",
        "        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
        "\n",
        "        # Get top predictions\n",
        "        values, indices = similarity[0].topk(5)\n",
        "\n",
        "    return [(imagenet_classes[idx], values[i].item()) for i, idx in enumerate(indices)]\n",
        "\n",
        "\n",
        "\n",
        "imagenet_preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Function to make predictions with ImageNet pretrained model\n",
        "def predict_with_imagenet(image_path):\n",
        "    # Load and preprocess image\n",
        "    image = imagenet_preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = imagenet_model(image)\n",
        "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n",
        "        # Get top predictions\n",
        "        values, indices = probabilities.topk(5)\n",
        "\n",
        "    return [(imagenet_classes[idx], values[i].item()) for i, idx in enumerate(indices)]\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\nTesting models on sample image:\")\n",
        "\n",
        "for path in os.listdir('sample_images'):\n",
        "\n",
        "    img_path = os.path.join('sample_images', path)\n",
        "    print(f\"Image: {img_path}\")\n",
        "    clip_predictions = predict_with_clip(img_path)\n",
        "    print(\"CLIP top-5 predictions:\")\n",
        "    for class_name, score in clip_predictions:\n",
        "        print(f\"  {class_name}: {score:.4f}\")\n",
        "\n",
        "    imagenet_predictions = predict_with_imagenet(img_path)\n",
        "    print(\"\\nImageNet top-5 predictions:\")\n",
        "    for class_name, score in imagenet_predictions:\n",
        "        print(f\"  {class_name}: {score:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvbuxwfa5Oue",
        "outputId": "972d45a4-43d0-48ef-ad28-482c263324d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Predictions for: agama ---\n",
            "CLIP Top-5:\n"
          ]
        },
        {
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 5.76 GiB of which 2.81 MiB is free. Process 6995 has 2.07 GiB memory in use. Process 25710 has 2.78 GiB memory in use. Including non-PyTorch memory, this process has 912.00 MiB memory in use. Of the allocated memory 578.10 MiB is allocated by PyTorch, and 223.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Predictions for: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCLIP Top-5:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mcls\u001b[39m, score \u001b[38;5;129;01min\u001b[39;00m \u001b[43mpredict_with_clip\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mImageNet RN50 Top-5:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mpredict_with_clip\u001b[39m\u001b[34m(image_path)\u001b[39m\n\u001b[32m     15\u001b[39m image_features = clip_model.encode_image(image)\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Get text features for all ImageNet classes\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m text_features = \u001b[43mclip_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Normalize features\u001b[39;00m\n\u001b[32m     21\u001b[39m image_features = image_features / image_features.norm(dim=-\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/clip/model.py:348\u001b[39m, in \u001b[36mCLIP.encode_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    346\u001b[39m x = x + \u001b[38;5;28mself\u001b[39m.positional_embedding.type(\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m    347\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m348\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    349\u001b[39m x = x.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# LND -> NLD\u001b[39;00m\n\u001b[32m    350\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_final(x).type(\u001b[38;5;28mself\u001b[39m.dtype)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/clip/model.py:203\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mresblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    249\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/clip/model.py:190\u001b[39m, in \u001b[36mResidualAttentionBlock.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.attention(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mln_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    191\u001b[39m     x = x + \u001b[38;5;28mself\u001b[39m.mlp(\u001b[38;5;28mself\u001b[39m.ln_2(x))\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/clip/model.py:162\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch.Tensor):\n\u001b[32m    161\u001b[39m     orig_type = x.dtype\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m     ret = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.type(orig_type)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/modules/normalization.py:217\u001b[39m, in \u001b[36mLayerNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/pythonapr25/lib/python3.12/site-packages/torch/nn/functional.py:2910\u001b[39m, in \u001b[36mlayer_norm\u001b[39m\u001b[34m(input, normalized_shape, weight, bias, eps)\u001b[39m\n\u001b[32m   2900\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[32m   2901\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m   2902\u001b[39m         layer_norm,\n\u001b[32m   2903\u001b[39m         (\u001b[38;5;28minput\u001b[39m, weight, bias),\n\u001b[32m   (...)\u001b[39m\u001b[32m   2908\u001b[39m         eps=eps,\n\u001b[32m   2909\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2910\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2911\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\n\u001b[32m   2912\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 152.00 MiB. GPU 0 has a total capacity of 5.76 GiB of which 2.81 MiB is free. Process 6995 has 2.07 GiB memory in use. Process 25710 has 2.78 GiB memory in use. Including non-PyTorch memory, this process has 912.00 MiB memory in use. Of the allocated memory 578.10 MiB is allocated by PyTorch, and 223.90 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "test_images = [\n",
        "    # (\"ladybug.jpg\", \"ladybug\"),\n",
        "    # (\"tennis_ball.jpg\", \"tennis_ball\"),\n",
        "    # (\"golden_retreiver.jpg\", \"golden retriever\"),\n",
        "    # (\"laptop.jpg\", \"laptop\"),\n",
        "    (\"counteranimal/42 agama/counter-tree/medium - 2023-09-08T135742.071.jpeg\", \"agama\"),\n",
        "]\n",
        "\n",
        "for img_path, label in test_images:\n",
        "    print(f\"\\n--- Predictions for: {label} ---\")\n",
        "\n",
        "    print(\"CLIP Top-5:\")\n",
        "    for cls, score in predict_with_clip(img_path):\n",
        "        print(f\"  {cls} ({score:.4f})\")\n",
        "\n",
        "    print(\"ImageNet RN50 Top-5:\")\n",
        "    for cls, score in predict_with_imagenet(img_path):\n",
        "        print(f\"  {cls} ({score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "nofe2GGGIYzG",
        "outputId": "7f72e587-5da7-4987-f9e8-a3a2818bb729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing folder: 293 cheetah, chetah, Acinonyx jubatus (GT: 293, cheetah)\n",
            "  Processing subfolder: counteranimal/293 cheetah, chetah, Acinonyx jubatus\n",
            "  Processing subfolder: counteranimal/293 cheetah, chetah, Acinonyx jubatus/counter-tree\n",
            "\n",
            "Processing folder: 9 ostrich, Struthio camelus (GT: 9, ostrich)\n",
            "  Processing subfolder: counteranimal/9 ostrich, Struthio camelus\n",
            "  Processing subfolder: counteranimal/9 ostrich, Struthio camelus/counter-water\n",
            "\n",
            "Processing folder: 102 echidna, spiny anteater, anteater (GT: 102, echidna)\n",
            "  Processing subfolder: counteranimal/102 echidna, spiny anteater, anteater\n",
            "  Processing subfolder: counteranimal/102 echidna, spiny anteater, anteater/counter-tree\n",
            "\n",
            "Processing folder: 81 ptarmigan (GT: 81, ptarmigan)\n",
            "  Processing subfolder: counteranimal/81 ptarmigan\n",
            "  Processing subfolder: counteranimal/81 ptarmigan/counter-grass\n",
            "\n",
            "Processing folder: 89 sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita (GT: 89, sulphur-crested_cockatoo)\n",
            "  Processing subfolder: counteranimal/89 sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita\n",
            "  Processing subfolder: counteranimal/89 sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita/counter-grass\n",
            "\n",
            "Processing folder: 277 red fox, Vulpes vulpes (GT: 277, red_fox)\n",
            "  Processing subfolder: counteranimal/277 red fox, Vulpes vulpes\n",
            "  Processing subfolder: counteranimal/277 red fox, Vulpes vulpes/counter-road\n",
            "\n",
            "Processing folder: 144 pelican (GT: 144, pelican)\n",
            "  Processing subfolder: counteranimal/144 pelican\n",
            "\n",
            "Processing folder: 33 loggerhead, loggerhead turtle, Caretta caretta (GT: 33, loggerhead)\n",
            "  Processing subfolder: counteranimal/33 loggerhead, loggerhead turtle, Caretta caretta\n",
            "  Processing subfolder: counteranimal/33 loggerhead, loggerhead turtle, Caretta caretta/counter-not in water\n",
            "\n",
            "Processing folder: 37 box turtle, box tortoise (GT: 37, box_turtle)\n",
            "  Processing subfolder: counteranimal/37 box turtle, box tortoise\n",
            "  Processing subfolder: counteranimal/37 box turtle, box tortoise/counter-earth\n",
            "CLIP fails but RN-50 passes: counteranimal/37 box turtle, box tortoise/counter-earth/medium - 2023-09-08T110208.363.jpeg\n",
            "\n",
            "Processing folder: 360 otter (GT: 360, otter)\n",
            "  Processing subfolder: counteranimal/360 otter\n",
            "  Processing subfolder: counteranimal/360 otter/counter-tree\n",
            "CLIP fails but RN-50 passes: counteranimal/360 otter/counter-tree/217.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/360 otter/counter-tree/amgkmelrmhaekmhklermhlakemharaergh.jpeg\n",
            "\n",
            "Processing folder: 275 African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus (GT: 275, african_hunting_dog)\n",
            "  Processing subfolder: counteranimal/275 African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus\n",
            "  Processing subfolder: counteranimal/275 African hunting dog, hyena dog, Cape hunting dog, Lycaon pictus/counter-tree\n",
            "\n",
            "Processing folder: 79 centipede (GT: 79, centipede)\n",
            "  Processing subfolder: counteranimal/79 centipede\n",
            "  Processing subfolder: counteranimal/79 centipede/counter-tree and grass\n",
            "\n",
            "Processing folder: 276 hyena, hyaena (GT: 276, hyena)\n",
            "  Processing subfolder: counteranimal/276 hyena, hyaena\n",
            "  Processing subfolder: counteranimal/276 hyena, hyaena/counter-road\n",
            "CLIP fails but RN-50 passes: counteranimal/276 hyena, hyaena/counter-road/medium - 2023-09-27T221047.394.jpeg\n",
            "CLIP fails but RN-50 passes: counteranimal/276 hyena, hyaena/counter-road/medium - 2023-09-27T220155.181.jpeg\n",
            "\n",
            "Processing folder: 56 king snake, kingsnake (GT: 56, king_snake)\n",
            "  Processing subfolder: counteranimal/56 king snake, kingsnake\n",
            "  Processing subfolder: counteranimal/56 king snake, kingsnake/counter-grass\n",
            "CLIP fails but RN-50 passes: counteranimal/56 king snake, kingsnake/counter-grass/medium - 2023-09-11T185649.255.jpeg\n",
            "CLIP fails but RN-50 passes: counteranimal/56 king snake, kingsnake/counter-grass/medium (69).jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/56 king snake, kingsnake/counter-grass/medium - 2023-09-11T185956.290.jpeg\n",
            "\n",
            "Processing folder: 49 African crocodile, Nile crocodile, Crocodylus niloticus (GT: 49, african_crocodile)\n",
            "  Processing subfolder: counteranimal/49 African crocodile, Nile crocodile, Crocodylus niloticus\n",
            "  Processing subfolder: counteranimal/49 African crocodile, Nile crocodile, Crocodylus niloticus/counter-grass\n",
            "\n",
            "Processing folder: 71 scorpion (GT: 71, scorpion)\n",
            "  Processing subfolder: counteranimal/71 scorpion\n",
            "  Processing subfolder: counteranimal/71 scorpion/counter-outdoor\n",
            "RN-50 fails but CLIP passes: counteranimal/71 scorpion/counter-outdoor/medium - 2023-09-18T124302.534.jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/71 scorpion/counter-outdoor/medium - 2023-09-18T125445.434.jpeg\n",
            "\n",
            "Processing folder: 54 hognose snake, puff adder, sand viper (GT: 54, hognose_snake)\n",
            "  Processing subfolder: counteranimal/54 hognose snake, puff adder, sand viper\n",
            "  Processing subfolder: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass\n",
            "CLIP fails but RN-50 passes: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T165237.410.jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T165738.745.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T165240.560.jpeg\n",
            "CLIP fails but RN-50 passes: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium (29).jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T011036.217.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium (37).jpg\n",
            "\n",
            "Processing folder: 100 black swan, Cygnus atratus (GT: 100, black_swan)\n",
            "  Processing subfolder: counteranimal/100 black swan, Cygnus atratus\n",
            "  Processing subfolder: counteranimal/100 black swan, Cygnus atratus/counter-ground\n",
            "\n",
            "Processing folder: 279 Arctic fox, white fox, Alopex lagopus (GT: 279, arctic_fox)\n",
            "  Processing subfolder: counteranimal/279 Arctic fox, white fox, Alopex lagopus\n",
            "  Processing subfolder: counteranimal/279 Arctic fox, white fox, Alopex lagopus/counter-grass\n",
            "\n",
            "Processing folder: 76 tarantula (GT: 76, tarantula)\n",
            "  Processing subfolder: counteranimal/76 tarantula\n",
            "  Processing subfolder: counteranimal/76 tarantula/counter-grass\n",
            "\n",
            "Processing folder: 291 lion, king of beasts, Panthera leo (GT: 291, lion)\n",
            "  Processing subfolder: counteranimal/291 lion, king of beasts, Panthera leo\n",
            "  Processing subfolder: counteranimal/291 lion, king of beasts, Panthera leo/counter-tree\n",
            "CLIP fails but RN-50 passes: counteranimal/291 lion, king of beasts, Panthera leo/counter-tree/medium - 2023-10-01T102650.028.jpg\n",
            "CLIP fails but RN-50 passes: counteranimal/291 lion, king of beasts, Panthera leo/counter-tree/jaigjroijgoaejrgoiarejaergar.jpeg\n",
            "\n",
            "Processing folder: 58 water snake (GT: 58, water_snake)\n",
            "  Processing subfolder: counteranimal/58 water snake\n",
            "  Processing subfolder: counteranimal/58 water snake/counter-ground\n",
            "CLIP fails but RN-50 passes: counteranimal/58 water snake/counter-ground/medium - 2023-09-12T164603.760.jpg\n",
            "CLIP fails but RN-50 passes: counteranimal/58 water snake/counter-ground/medium (49).jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/58 water snake/counter-ground/medium - 2023-09-12T164600.316.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/58 water snake/counter-ground/medium - 2023-09-12T220544.514.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/58 water snake/counter-ground/medium - 2023-09-12T162239.536.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/58 water snake/counter-ground/medium - 2023-09-12T163434.464.jpeg\n",
            "\n",
            "Processing folder: 10 brambling, Fringilla montifringilla (GT: 10, brambling)\n",
            "  Processing subfolder: counteranimal/10 brambling, Fringilla montifringilla\n",
            "  Processing subfolder: counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue\n",
            "CLIP fails but RN-50 passes: counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (27).jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (32).jpeg\n",
            "CLIP fails but RN-50 passes: counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (26).jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (16).jpeg\n",
            "\n",
            "Processing folder: 39 common iguana, iguana, Iguana iguana (GT: 39, common_iguana)\n",
            "  Processing subfolder: counteranimal/39 common iguana, iguana, Iguana iguana\n",
            "  Processing subfolder: counteranimal/39 common iguana, iguana, Iguana iguana/counter-shrub\n",
            "CLIP fails but RN-50 passes: counteranimal/39 common iguana, iguana, Iguana iguana/counter-shrub/medium - 2023-09-08T125451.193.jpg\n",
            "CLIP fails but RN-50 passes: counteranimal/39 common iguana, iguana, Iguana iguana/counter-shrub/medium - 2023-09-08T124328.446.jpg\n",
            "\n",
            "Processing folder: 357 mink (GT: 357, mink)\n",
            "  Processing subfolder: counteranimal/357 mink\n",
            "  Processing subfolder: counteranimal/357 mink/counter-above water\n",
            "\n",
            "Processing folder: 30 bullfrog, Rana catesbeiana (GT: 30, bullfrog)\n",
            "  Processing subfolder: counteranimal/30 bullfrog, Rana catesbeiana\n",
            "  Processing subfolder: counteranimal/30 bullfrog, Rana catesbeiana/counter-not in water\n",
            "CLIP fails but RN-50 passes: counteranimal/30 bullfrog, Rana catesbeiana/counter-not in water/medium (54).jpeg\n",
            "\n",
            "Processing folder: 290 jaguar, panther, Panthera onca, Felis onca (GT: 290, jaguar)\n",
            "  Processing subfolder: counteranimal/290 jaguar, panther, Panthera onca, Felis onca\n",
            "  Processing subfolder: counteranimal/290 jaguar, panther, Panthera onca, Felis onca/counter-tree\n",
            "CLIP fails but RN-50 passes: counteranimal/290 jaguar, panther, Panthera onca, Felis onca/counter-tree/medium - 2023-10-01T101625.232.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/290 jaguar, panther, Panthera onca, Felis onca/counter-tree/medium - 2023-10-01T101627.186.jpeg\n",
            "\n",
            "Processing folder: 41 whiptail, whiptail lizard (GT: 41, whiptail)\n",
            "  Processing subfolder: counteranimal/41 whiptail, whiptail lizard\n",
            "  Processing subfolder: counteranimal/41 whiptail, whiptail lizard/counter-hand\n",
            "RN-50 fails but CLIP passes: counteranimal/41 whiptail, whiptail lizard/counter-hand/trjhkroptkjporkphortkphotkphrthrt.jpeg\n",
            "\n",
            "Processing folder: 70 harvestman, daddy longlegs, Phalangium opilio (GT: 70, harvestman)\n",
            "  Processing subfolder: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio\n",
            "  Processing subfolder: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock\n",
            "RN-50 fails but CLIP passes: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T100728.216.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium (67).jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T104842.074.jpg\n",
            "CLIP fails but RN-50 passes: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T105229.884.jpeg\n",
            "CLIP fails but RN-50 passes: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T105134.515.jpeg\n",
            "RN-50 fails but CLIP passes: counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T104851.876.jpeg\n",
            "\n",
            "Processing folder: 83 prairie chicken, prairie grouse, prairie fowl (GT: 83, prairie_chicken)\n",
            "  Processing subfolder: counteranimal/83 prairie chicken, prairie grouse, prairie fowl\n",
            "  Processing subfolder: counteranimal/83 prairie chicken, prairie grouse, prairie fowl/counter-snow\n",
            "\n",
            "Processing folder: 305 dung beetle (GT: 305, dung_beetle)\n",
            "  Processing subfolder: counteranimal/305 dung beetle\n",
            "  Processing subfolder: counteranimal/305 dung beetle/counter-human or hand\n",
            "RN-50 fails but CLIP passes: counteranimal/305 dung beetle/counter-human or hand/109.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/305 dung beetle/counter-human or hand/165.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/305 dung beetle/counter-human or hand/1.jpg\n",
            "CLIP fails but RN-50 passes: counteranimal/305 dung beetle/counter-human or hand/510.jpg\n",
            "RN-50 fails but CLIP passes: counteranimal/305 dung beetle/counter-human or hand/176.jpg\n",
            "\n",
            "Processing folder: 133 bittern (GT: 133, bittern)\n",
            "  Processing subfolder: counteranimal/133 bittern\n",
            "  Processing subfolder: counteranimal/133 bittern/counter-tree\n",
            "RN-50 fails but CLIP passes: counteranimal/133 bittern/counter-tree/dtyjdtyukdtyjdtyjdtykjdtjdtyjtdyjt.jpg\n",
            "\n",
            "--- CLIP fails but RN-50 passes (Total: 21) ---\n",
            "counteranimal/37 box turtle, box tortoise/counter-earth/medium - 2023-09-08T110208.363.jpeg\n",
            "counteranimal/360 otter/counter-tree/217.jpg\n",
            "counteranimal/276 hyena, hyaena/counter-road/medium - 2023-09-27T221047.394.jpeg\n",
            "counteranimal/276 hyena, hyaena/counter-road/medium - 2023-09-27T220155.181.jpeg\n",
            "counteranimal/56 king snake, kingsnake/counter-grass/medium - 2023-09-11T185649.255.jpeg\n",
            "counteranimal/56 king snake, kingsnake/counter-grass/medium (69).jpg\n",
            "counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T165237.410.jpeg\n",
            "counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium (29).jpeg\n",
            "counteranimal/291 lion, king of beasts, Panthera leo/counter-tree/medium - 2023-10-01T102650.028.jpg\n",
            "counteranimal/291 lion, king of beasts, Panthera leo/counter-tree/jaigjroijgoaejrgoiarejaergar.jpeg\n",
            "counteranimal/58 water snake/counter-ground/medium - 2023-09-12T164603.760.jpg\n",
            "counteranimal/58 water snake/counter-ground/medium (49).jpeg\n",
            "counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (27).jpg\n",
            "counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (26).jpeg\n",
            "counteranimal/39 common iguana, iguana, Iguana iguana/counter-shrub/medium - 2023-09-08T125451.193.jpg\n",
            "counteranimal/39 common iguana, iguana, Iguana iguana/counter-shrub/medium - 2023-09-08T124328.446.jpg\n",
            "counteranimal/30 bullfrog, Rana catesbeiana/counter-not in water/medium (54).jpeg\n",
            "counteranimal/290 jaguar, panther, Panthera onca, Felis onca/counter-tree/medium - 2023-10-01T101625.232.jpg\n",
            "counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T105229.884.jpeg\n",
            "counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T105134.515.jpeg\n",
            "counteranimal/305 dung beetle/counter-human or hand/510.jpg\n",
            "\n",
            "--- RN-50 fails but CLIP passes (Total: 25) ---\n",
            "counteranimal/360 otter/counter-tree/amgkmelrmhaekmhklermhlakemharaergh.jpeg\n",
            "counteranimal/56 king snake, kingsnake/counter-grass/medium - 2023-09-11T185956.290.jpeg\n",
            "counteranimal/71 scorpion/counter-outdoor/medium - 2023-09-18T124302.534.jpeg\n",
            "counteranimal/71 scorpion/counter-outdoor/medium - 2023-09-18T125445.434.jpeg\n",
            "counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T165738.745.jpg\n",
            "counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T165240.560.jpeg\n",
            "counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium - 2023-09-11T011036.217.jpg\n",
            "counteranimal/54 hognose snake, puff adder, sand viper/counter-grass/medium (37).jpg\n",
            "counteranimal/58 water snake/counter-ground/medium - 2023-09-12T164600.316.jpg\n",
            "counteranimal/58 water snake/counter-ground/medium - 2023-09-12T220544.514.jpg\n",
            "counteranimal/58 water snake/counter-ground/medium - 2023-09-12T162239.536.jpg\n",
            "counteranimal/58 water snake/counter-ground/medium - 2023-09-12T163434.464.jpeg\n",
            "counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (32).jpeg\n",
            "counteranimal/10 brambling, Fringilla montifringilla/counter-white or blue/medium (16).jpeg\n",
            "counteranimal/290 jaguar, panther, Panthera onca, Felis onca/counter-tree/medium - 2023-10-01T101627.186.jpeg\n",
            "counteranimal/41 whiptail, whiptail lizard/counter-hand/trjhkroptkjporkphortkphotkphrthrt.jpeg\n",
            "counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T100728.216.jpg\n",
            "counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium (67).jpeg\n",
            "counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T104842.074.jpg\n",
            "counteranimal/70 harvestman, daddy longlegs, Phalangium opilio/counter-rock/medium - 2023-09-18T104851.876.jpeg\n",
            "counteranimal/305 dung beetle/counter-human or hand/109.jpg\n",
            "counteranimal/305 dung beetle/counter-human or hand/165.jpg\n",
            "counteranimal/305 dung beetle/counter-human or hand/1.jpg\n",
            "counteranimal/305 dung beetle/counter-human or hand/176.jpg\n",
            "counteranimal/133 bittern/counter-tree/dtyjdtyukdtyjdtyjdtykjdtjdtyjtdyjt.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import difflib\n",
        "import cv2  # Only used to check number of channels\n",
        "\n",
        "# from your_module import predict_with_clip, predict_with_imagenet\n",
        "\n",
        "dataset_root = \"counteranimal\"\n",
        "imagenet_classes_file = \"imagenet_classes.txt\"\n",
        "\n",
        "with open(imagenet_classes_file, \"r\") as f:\n",
        "    imagenet_classes = [line.strip().lower() for line in f.readlines()]\n",
        "\n",
        "def match_imagenet_label(folder_name, classes, cutoff=0.6):\n",
        "    parts = folder_name.split(' ', 1)\n",
        "    if len(parts) < 2:\n",
        "        raise ValueError(f\"Unexpected folder name format: {folder_name}\")\n",
        "    synonyms = [syn.strip().lower() for syn in parts[1].split(',')]\n",
        "    for syn in synonyms:\n",
        "        if not syn:\n",
        "            continue\n",
        "        matches = difflib.get_close_matches(syn, classes, n=1, cutoff=cutoff)\n",
        "        if matches:\n",
        "            return matches[0]\n",
        "    raise ValueError(f\"No close Imagenet match for any of {synonyms} (folder {folder_name})\")\n",
        "\n",
        "clip_fails_imagenet_passes = []\n",
        "imagenet_fails_clip_passes = []\n",
        "\n",
        "clip_fail_rn50_pass_count = 0\n",
        "rn50_fail_clip_pass_count = 0\n",
        "\n",
        "# Iterate through folders\n",
        "for class_folder in os.listdir(dataset_root):\n",
        "    class_path = os.path.join(dataset_root, class_folder)\n",
        "    if not os.path.isdir(class_path):\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        gt_label = match_imagenet_label(class_folder, imagenet_classes)\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        continue\n",
        "\n",
        "    # animals = ['agama', 'beaver', 'bighorn', \"black grouse\", \"bulbul\", \"cicada\", \"flamingo\", \"loggerhead\", \"water ouzel\"]\n",
        "    # if not any(gt_label.endswith(animal) for animal in animals):\n",
        "    #     continue\n",
        "\n",
        "    print(f\"\\nProcessing folder: {class_folder} (GT: {gt_label})\")\n",
        "    for root, _, files in os.walk(class_path):\n",
        "        clip_fail_rn50_pass_count = 0\n",
        "        rn50_fail_clip_pass_count = 0\n",
        "        print(f\"  Processing subfolder: {root}\")\n",
        "        for fname in files:\n",
        "            if clip_fail_rn50_pass_count >= 2 and rn50_fail_clip_pass_count >= 4:\n",
        "                break\n",
        "\n",
        "            if not fname.lower().endswith(('.jpg', '.jpeg')):\n",
        "                continue\n",
        "\n",
        "            img_path = os.path.join(root, fname)\n",
        "            img = cv2.imread(img_path)\n",
        "            if img is None or len(img.shape) != 3 or img.shape[2] != 3:\n",
        "                continue  # Not a 3-channel image\n",
        "\n",
        "            clip_preds = [cls.lower() for cls, _ in predict_with_clip(img_path)]\n",
        "            rn50_preds = [cls.lower() for cls, _ in predict_with_imagenet(img_path)]\n",
        "\n",
        "            clip_pass = gt_label in clip_preds\n",
        "            rn50_pass = gt_label in rn50_preds\n",
        "\n",
        "            # print(f\"Image: {img_path}\")\n",
        "            # print(f\"  CLIP predictions: {clip_preds} (Pass: {clip_pass})\")\n",
        "            # print(f\"  RN-50 predictions: {rn50_preds} (Pass: {rn50_pass})\")\n",
        "            if (clip_fail_rn50_pass_count < 2) and (not clip_pass and rn50_pass):\n",
        "                clip_fails_imagenet_passes.append(img_path)\n",
        "                clip_fail_rn50_pass_count += 1\n",
        "                print(f\"CLIP fails but RN-50 passes: {img_path}\")\n",
        "\n",
        "            elif (rn50_fail_clip_pass_count < 4) and (not rn50_pass and clip_pass):\n",
        "                imagenet_fails_clip_passes.append(img_path)\n",
        "                rn50_fail_clip_pass_count += 1\n",
        "                print(f\"RN-50 fails but CLIP passes: {img_path}\")\n",
        "\n",
        "            if clip_fail_rn50_pass_count >= 2 and rn50_fail_clip_pass_count >= 4:\n",
        "                break\n",
        "\n",
        "def print_results(title, paths):\n",
        "    print(f\"\\n--- {title} (Total: {len(paths)}) ---\")\n",
        "    for p in paths:\n",
        "        print(p)\n",
        "\n",
        "print_results(\"CLIP fails but RN-50 passes\", clip_fails_imagenet_passes)\n",
        "print_results(\"RN-50 fails but CLIP passes\", imagenet_fails_clip_passes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DmhnhL85IYzH",
        "outputId": "309d0638-e458-48c4-c7e7-4a30250f21f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['35, mud_turtle', '63, indian_cobra', '38, banded_gecko', '78, tick', '44, alligator_lizard']\n",
            "['37, box_turtle', '36, terrapin', '38, banded_gecko', '42, agama', '35, mud_turtle']\n",
            "False\n",
            "True\n",
            "CLIP fails but RN-50 passes: counteranimal/42 agama/counter-tree/medium - 2023-09-08T135742.071.jpeg\n"
          ]
        }
      ],
      "source": [
        "img_path = \"counteranimal/42 agama/counter-tree/medium - 2023-09-08T135742.071.jpeg\"\n",
        "clip_preds = [cls.lower() for cls, _ in predict_with_clip(img_path)]\n",
        "rn50_preds = [cls.lower() for cls, _ in predict_with_imagenet(img_path)]\n",
        "print(clip_preds)\n",
        "print(rn50_preds)\n",
        "gt_label = \"42, agama\"\n",
        "\n",
        "clip_pass = gt_label in clip_preds\n",
        "rn50_pass = gt_label in rn50_preds\n",
        "\n",
        "print(clip_pass)\n",
        "print(rn50_pass)\n",
        "\n",
        "# Case: CLIP fails but RN-50 passes\n",
        "if not clip_pass and rn50_pass:\n",
        "    print(f\"CLIP fails but RN-50 passes: {img_path}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA628wBGvJhh"
      },
      "outputs": [],
      "source": [
        "#############################################################\n",
        "# Task 4: CLIP vs ImageNet pretraining\n",
        "#############################################################\n",
        "\n",
        "# Select 10 diverse ImageNet classes\n",
        "selected_classes = [\n",
        "    \"goldfish\",       # Aquatic animal\n",
        "    \"airliner\",       # Vehicle\n",
        "    \"grand_piano\",    # Musical instrument\n",
        "    \"coffee_mug\",     # Household object\n",
        "    \"golden_retriever\", # Dog breed\n",
        "    \"monarch\",        # Butterfly/insect\n",
        "    \"ambulance\",      # Emergency vehicle\n",
        "    \"volcano\",        # Natural formation\n",
        "    \"strawberry\",     # Fruit\n",
        "    \"desktop_computer\" # Electronics\n",
        "]\n",
        "\n",
        "def download_test_images():\n",
        "    \"\"\"Download test images for our comparison\"\"\"\n",
        "    # Create directories for images\n",
        "    os.makedirs(\"test_images\", exist_ok=True)\n",
        "    os.makedirs(\"test_images/clip_better\", exist_ok=True)\n",
        "    os.makedirs(\"test_images/imagenet_better\", exist_ok=True)\n",
        "\n",
        "download_test_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8Ifp50jvJhh"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import clip\n",
        "import time\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import subprocess\n",
        "import copy\n",
        "\n",
        "# Load the original FP32 model\n",
        "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# clip_visual_fp32, preprocess = clip.load(\"RN50\", device=device)\n",
        "\n",
        "# model_fp32 = model_fp32.visual.float()\n",
        "\n",
        "# # Create FP16 version\n",
        "# model_fp16 = copy.deepcopy(model_fp32).half()\n",
        "\n",
        "# # model_fp16 = copy.deepcopy(model_fp32).to(device)\n",
        "# # model_fp32 = model_fp32.to(device).float()\n",
        "# # Prepare test images (5 images from different classes)\n",
        "image_paths = [\n",
        "    'sample_images/bee_eater.JPEG'\n",
        "]\n",
        "\n",
        "def load_clip_fp32():\n",
        "    clip_model, preprocess = clip.load(\"RN50\", device=device)\n",
        "    return clip_model.visual.float()\n",
        "\n",
        "def load_clip_fp16():\n",
        "    clip_model, preprocess = clip.load(\"RN50\", device=device)\n",
        "    return clip_model.visual.half()\n",
        "\n",
        "images = [clip_preprocess(Image.open(path)).unsqueeze(0).to(device) for path in image_paths]\n",
        "\n",
        "def test_fp16_performance_improved():\n",
        "    print(\"\\nImproved FP16 vs FP32 Performance Test:\")\n",
        "\n",
        "    # Print device info\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
        "        print(f\"CUDA Version: {torch.version.cuda}\")\n",
        "\n",
        "    print(f\"PyTorch Version: {torch.__version__}\")\n",
        "\n",
        "    # Sample image for testing\n",
        "    sample_image = clip_preprocess(Image.open('sample_images/bee_eater.JPEG')).unsqueeze(0).to(device)\n",
        "    sample_image = sample_image.repeat(64, 1, 1, 1)\n",
        "    # Create proper copies to avoid interference\n",
        "    clip_visual_fp32 = load_clip_fp32()\n",
        "    torch.cuda.empty_cache()  # Clear cache between model loads\n",
        "\n",
        "    clip_visual_fp16 = load_clip_fp16()\n",
        "    torch.cuda.empty_cache()  # Clear cache between model loads\n",
        "\n",
        "    print(\"Model-F32 dtype:\", next(clip_visual_fp32.parameters()).dtype)  # Should be torch.float32\n",
        "    print(\"Model-F16 dtype:\", next(clip_visual_fp16.parameters()).dtype)\n",
        "\n",
        "    # Warm-up runs - more extensive\n",
        "    print(\"Warming up models...\")\n",
        "    with torch.no_grad():\n",
        "        for _ in range(20):\n",
        "            _ = clip_visual_fp32(sample_image.to(torch.float32))\n",
        "\n",
        "        for _ in range(20):\n",
        "            _ = clip_visual_fp16(sample_image.to(torch.float16))\n",
        "\n",
        "    # Memory test first (before timing tests)\n",
        "    print(\"\\nMemory analysis:\")\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Isolate FP32 memory usage with a clear pattern\n",
        "    with torch.autocast(device_type='cuda', dtype=torch.float32):\n",
        "        with torch.no_grad():\n",
        "            _ = clip_visual_fp32(sample_image.to(torch.float32))\n",
        "    fp32_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "\n",
        "    # Clear completely between tests\n",
        "    del clip_visual_fp32\n",
        "    torch.cuda.empty_cache()\n",
        "    torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    # Load a fresh FP16 model\n",
        "    clip_visual_fp16 = load_clip_fp16()\n",
        "\n",
        "    # Isolate FP16 memory usage\n",
        "    with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "        with torch.no_grad():\n",
        "            _ = clip_visual_fp16(sample_image.to(torch.float16))\n",
        "        fp16_memory = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "\n",
        "    print(f\"FP32 peak memory usage: {fp32_memory:.2f} MB\")\n",
        "    print(f\"FP16 peak memory usage: {fp16_memory:.2f} MB\")\n",
        "    print(f\"Memory change: {(fp16_memory/fp32_memory - 1) * 100:.2f}%\")\n",
        "\n",
        "    # Reload models for timing tests\n",
        "    clip_visual_fp32 = load_clip_fp32()\n",
        "    clip_visual_fp16 = load_clip_fp16()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Timing with torch.cuda.Event for more accurate GPU timing\n",
        "    print(\"\\nTiming tests (using CUDA events for accuracy):\")\n",
        "\n",
        "    # Function for accurate timing\n",
        "    def measure_time(model, input_tensor, precision, iterations=100):\n",
        "        times = []\n",
        "        for _ in range(iterations):\n",
        "            if torch.cuda.is_available():\n",
        "                start_event = torch.cuda.Event(enable_timing=True)\n",
        "                end_event = torch.cuda.Event(enable_timing=True)\n",
        "\n",
        "                # Synchronize before starting\n",
        "                torch.cuda.synchronize()\n",
        "                start_event.record()\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    _ = model(input_tensor)\n",
        "\n",
        "                end_event.record()\n",
        "                torch.cuda.synchronize()\n",
        "\n",
        "                # Convert to milliseconds, then to seconds\n",
        "                times.append(start_event.elapsed_time(end_event) / 1000)\n",
        "            else:\n",
        "                # Fallback to time.time() for CPU\n",
        "                start = time.time()\n",
        "                with torch.no_grad():\n",
        "                    _ = model(input_tensor)\n",
        "                times.append(time.time() - start)\n",
        "\n",
        "        return np.mean(times), np.std(times)\n",
        "\n",
        "    # Measure FP32\n",
        "    fp32_mean, fp32_std = measure_time(\n",
        "        clip_visual_fp32,\n",
        "        sample_image.to(torch.float32),\n",
        "        \"fp32\"\n",
        "    )\n",
        "\n",
        "    # Measure FP16\n",
        "    fp16_mean, fp16_std = measure_time(\n",
        "        clip_visual_fp16,\n",
        "        sample_image.to(torch.float16),\n",
        "        \"fp16\"\n",
        "    )\n",
        "\n",
        "    print(f\"FP32 inference time: {fp32_mean:.6f} ± {fp32_std:.6f} seconds\")\n",
        "    print(f\"FP16 inference time: {fp16_mean:.6f} ± {fp16_std:.6f} seconds\")\n",
        "    print(f\"Speed ratio (FP32/FP16): {fp32_mean/fp16_mean:.2f}x\")\n",
        "\n",
        "    # Output comparison\n",
        "    print(\"\\nOutput comparison:\")\n",
        "    with torch.no_grad():\n",
        "        # Get outputs\n",
        "        fp32_output = clip_visual_fp32(sample_image.to(torch.float32))\n",
        "        fp16_output = clip_visual_fp16(sample_image.to(torch.float16))\n",
        "\n",
        "        # Convert FP16 output to FP32 for comparison\n",
        "        fp16_as_fp32 = fp16_output.to(torch.float32)\n",
        "\n",
        "        # Calculate difference\n",
        "        abs_diff = torch.abs(fp32_output - fp16_as_fp32)\n",
        "        rel_diff = abs_diff / (torch.abs(fp32_output) + 1e-8)\n",
        "\n",
        "        print(f\"Mean absolute difference: {abs_diff.mean().item():.8f}\")\n",
        "        print(f\"Max absolute difference: {abs_diff.max().item():.8f}\")\n",
        "        print(f\"Mean relative difference: {rel_diff.mean().item():.8f}\")\n",
        "        print(f\"Max relative difference: {rel_diff.max().item():.8f}\")\n",
        "        print(f\"Are there significant differences? {'Yes' if abs_diff.max().item() > 0.01 else 'No'}\")\n",
        "\n",
        "test_fp16_performance_improved()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pythonapr25",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
